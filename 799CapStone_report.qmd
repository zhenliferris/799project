---
title: "Analysis for Dataset 'Infrared Thermography' "
subtitle: "799 DSAN CapStone Project"
author: "Zhen Li"
date: "2024-06-1"
format:
  pdf:
    documentclass: article
    csl: apa.csl
header-includes: |
  \usepackage{fancyhdr}
  \pagestyle{fancy}
  \fancyhead[L]{Analysis for dataset 'Infrared...' }
bibliography: references.bib
---




# Abstract:

In public gathering areas such as airports, train stations, hospitals, and schools, there is a high likelihood of becoming transmission hubs for infectious diseases. If potential patients can be quickly identified and further screened, it could greatly reduce the risk of spreading infectious diseases. Fever is typically a very common symptom of illness, especially for diseases that can severely threaten public health, such as the recently experienced global outbreak of the COVID19. 

When assessing fever symptoms, the oral temperature is the true value we aim to obtain. Measuring oral temperature for large crowds involves hygiene and time efficiency issues. Without direct contact with the human body, infrared thermography can quickly capture facial temperature. However, the measurement values of infrared thermography can be affected by other factors, thereby reducing the reference value of its readings. The goal of this project is to attempt to establish a good model that accurately predicts the oral temperature of individuals using quickly obtained facial temperature from certain areas and other easily measurable parameters. 

If the model can provide good predictive performance, it can preliminary and rapidly screen out individuals with fever symptoms for further examination, playing a very positive role in preventing the spread of pandemic diseases.


# Data Understanding:

-  **The data source**

The original dataset, FLIR_groups1and2.csv, is available for download at https://doi.org/10.13026/3bhc-9065 [@wang2023facial]. This file contains 1,020 observations and four rounds of measurements across 120 variables. For this project, which aims to develop a model for rapid screening, we have retained 32 variables from the first round of measurements, focusing on infrared thermography and environmental parameters, for our analysis. 


-  **Variables list**


As listed below, oral temperature is our target value. We aim to predict this target value based on the measurements of the feature (input) variables.


|Variable Name	|Role	|Type|	Description|	 
|:--------------|:------|:----------|:--------------|
|SubjectID|	ID|	Categorical	|	Subject ID|
|T_OR_Max1	|Target|	Continuous|		Oral temperature | 															
|Gender|	Feature|	Categorical	|Male or Female		 |
|Age	|Feature|	Categorical	|	Age ranges in categories |
|Ethnicity|	Feature	|Categorical	|	American Indian or Alaska Native, Asian, Black or African America, Hispanic/Latino, Multiracial, Native Hawaiian or other Pacific Islander, white.		 |
|T_atm|	Feature	|Continuous	|	Ambiant temperature		 |
|Humidity|	Feature	|Continuous	|	Relative humidity	 |
|Distance	|Feature	|Continuous	|	Distance between the subjects and the IRTs 		 |
|T_offset1|Feature|Continuous	|	Temperature difference between the set and measured blackbody temperature|
Max1R13_1	|Feature	|Continuous	|	Max value of a circle with diameter of 13 pixels from the right canthus point to the face centerline	 |
|Max1L13_1|	Feature	|Continuous	|	Max value of a circle with diameter of 13 pixels from the left canthus point to the face centerline		 |
|aveAllR13_1|	Feature|	Continuous	|	Average value of a circle with diameter of 13 pixels from the right canthus point to the face centerline |
|aveAllL13_1| Feature|	Continuous|		Average value of a circle with diameter of 13 pixels from the left canthus point to the face centerline	 |
|T_RC1|	Feature|	Continuous	|	Average temperature of the highest four pixels in a square of 24x24 pixels around the right canthus, with 2/3 toward the face center (dry area, 16x24 pixels) and 1/3 away from the face center (wet area, 8x24 pixels).		 |
|T_RC_Dry1|	Feature|	Continuous|		Average temperature of the highest four pixels in the right canthus dry area, a rectangle of 16x24 pixels.	 |
|T_RC_Wet1|	Feature|	Continuous	|	Average temperature of the highest four pixels in the right canthus wet area, a rectangle of 8x24 pixels.	 |
|T_RC_Max1|	Feature|	Continuous	|	Max value of a square of 24x24 pixels around the right canthus, with 2/3 toward the face center (dry area, 16x24 pixels) and 1/3 away from the face center (wet area, 8x24 pixels).		 |
 |T_LC1|	Feature	|Continuous	|	Average temperature of the highest four pixels in a square of 24x24 pixels around the left canthus, with 2/3 toward the face center (dry area, 16x24 pixels) and 1/3 away from the face center (wet area, 8x24 pixels).				|
|T_LC_Dry1|	Feature	|Continuous	|	Average temperature of the highest four pixels in the left canthus dry area, a rectangle of 16x24 pixels.	 |
|T_LC_Wet1|	Feature|	Continuous	|	Average temperature of the highest four pixels in the left canthus wet area, a rectangle of 16x24 pixels.	 |
|T_LC_Max1|	Feature|	Continuous	|	Max value of a circle with diameter of 13 pixels from the left canthus point to the face centerline	 |
|RCC1|	Feature|	Continuous	|	Average value of a square of 3x3 pixels centered at the right canthus point. |
|LCC1|	Feature	|Continuous	|	Average value of a square of 3x3 pixels centered at the left canthus point.	 |
|canthiMax1|Feature|	Continuous	|	Max value in the extended canthi area	 |
|T_FHCC1|Feature|	Continuous	|	Average temperature within Center point of forehead, a square of 3x3 pixels	 |
|T_FHRC1|Feature|	Continuous	| Average temperature within Right point of the forehead, a square of 3x3 pixels |
|T_FHLC1|Feature|	Continuous	|Average temperature within	Left point of the forehead, a square of 3x3 pixels |
|T_FHBC1|Feature|	Continuous	|Average temperature within	Bottom point of the forehead, a square of 3x3 pixels|
|T_FHTC1|Feature|	Continuous	|	Average temperature within Top point of the forehead, a square of 3x3 pixels	 |
|T_FH_Max1|Feature|	Continuous	|Maximum temperature within the extended
forehead area |
|T_FHC_Max1|Feature|	Continuous	|	Max value in the Center point of forehead, a square of 3x3 pixels	 |
|T_Max1|Feature|	Continuous	|	Maximum temperature within the whole face region |   



-  **Measure Guidance**


Figure 1 illustrates the methods used to measure all the temperatures. The image is provided by one of the authors who donated the original dataset.



*Figure 1*

[![Mesurement guidance01](C:/799CapStone_Z/facial_oral_temperature_data/_Figure1.png)]    

Figure 2 serves as a guide for taking measurements. Additionally, environmental parameters will be recorded as input variables.


*Figure 2*

[![Mesurement guidance02](C:/799CapStone_Z/facial_oral_temperature_data/JBO_25_9_097002_f001.png)]

# Data Prepariation:

-  **Load Necessary Library**

The code listed below includes all the R packages used in this analysis. Please note that some functions defined in different packages share the same names; the function in the last loaded package will be used by default. To use a specific package's function, use the '::' operator.


```{r warning=FALSE, message=FALSE}
library(tidyverse) #include "dplyr" "ggplot2"
library(Hmisc)
library(psych)
library(scales)
library(caTools) # seed
library(patchwork) # multiplot 
library(car) # vif
library(reshape2) # melt() to convert wide format to long format
library(caret)
library(glmnet)
library(randomForest)
```


- **Brief Look at the Selected Data**

Firstly, let's examine the selected variables for our analysis. As listed below, there are a total of 32 variables and 1,020 observations. We notice that some variables, such as 'T_offset1', have missing values. Additionally, some categorical data, like 'Age', have overlapping ranges such as '21-25', '26-30', and '21-30', which should be combined into a single group to reduce complexity.

```{r echo=FALSE, warning=FALSE, message=FALSE}
# Read the CSV file
file_path <- "C:/799CapStone_Z/facial_oral_temperature_data/FLIR_groups1and2.csv"
data <- read.csv(file_path, header = FALSE)
data_selected <- data %>% select(3:27, 29, 117:122)

colnames(data_selected) <- data_selected[3,]
data_selected <-data_selected[-(1:3),]
Hmisc::describe(data_selected)
```


- **Dealing the Missing Data and Combine Age Group**

After removing the missing values and combining age groups, the cleaned data consists of 32 variables and 1,001 observations. While the data quality has improved, some values still appear to be incorrect. For instance, the highest value of 'Distance' is 79, which seems unreasonable.


```{r echo=FALSE, warning=FALSE, message=FALSE}
# Convert all empty strings to NA
data_selected[data_selected == ""] <- NA

# Remove rows with any NA values
data_cleaned <- data_selected[complete.cases(data_selected), ]

# combine age 21-30 to one group 
data_cleaned$Age[data_cleaned$Age == "21-25" | data_cleaned$Age == "26-30"] <- "21-30"
data_cleaned$Age[data_cleaned$Age == ">60" ] <- "60+"
Hmisc::describe(data_cleaned)
```


- **Plot Categorical Data**

To better understand the data, we begin by plotting the categorical data.

1. The test subjects are fairly evenly distributed between males and females, with a ratio of 6:4.

2. The dataset is predominantly composed of individuals under 30, suggesting that the model built on this data may not be suitable for all age groups.

3. Similarly, the distribution of ethnic groups is unbalanced, indicating that the model based on this dataset may not be appropriate for all ethnicities.


```{r echo=FALSE, warning=FALSE, message=FALSE}
# Categorical data
catvars_plot <- names(data_cleaned)[27:29]
for (catvar in catvars_plot) {
  p <- ggplot(data_cleaned, aes_string(x = catvar, fill=catvar)) +
    geom_bar() +
    labs(title = paste("Bar chart grouped by", catvar), x = catvar, y = "Frequency")+
    coord_flip() +
    theme(legend.text = element_text(size = 8),
          legend.title = element_text(size = 10),
          axis.text.y = element_blank())
  
  print(p)
}

```


- **Process with Categorical Data to Numerical**

To build the model, we convert categorical data to numerical values.

1. We set "gender is female" as the baseline, meaning that when "gender is female," the value of Gender_M is zero.

2. Based on the plot presented above, we will not consider the age group.

3. For the same reason, we will not consider the ethnicity group.


```{r echo=FALSE, warning=FALSE, message=FALSE}
mydataframe <- data_cleaned %>% 
  mutate(
    # Encoding categorical variables
    # Gender_Female as base line
    Gender_M = ifelse(Gender == "Male", 1, 0),
    # # Age_less20 as base line
    # Age_less30 = ifelse(Age == "21-30", 1, 0),
    # Age_less40 = ifelse(Age == "31-40", 1, 0),
    # Age_less50 = ifelse(Age == "41-50", 1, 0),
    # Age_less60 = ifelse(Age == "51-60", 1, 0),
    # Age_greater60 = ifelse(Age == "60+", 1, 0),
    # # Ethnicity American Indian or Alaskan Native as base line
    # Ethnicity_A = ifelse(Ethnicity == "Asian", 1, 0),
    # Ethnicity_B = ifelse(Ethnicity == "Black or African-American", 1, 0),
    # Ethnicity_H = ifelse(Ethnicity == "Hispanic/Latino", 1, 0),
    # Ethnicity_M = ifelse(Ethnicity == "Multiracial", 1, 0),
    # Ethnicity_W = ifelse(Ethnicity == "White", 1, 0),
    # #variable_SD = scale(variable),
  )

# Removing columns: 'Gender', 'Age', and 'Ethnicity'
mydataframe <- mydataframe[, !(names(mydataframe) %in% c("Gender", "Age", "Ethnicity"))]  

# Ensure all variables are numerical
mydataframe <- mydataframe %>% 
  mutate_if(~ all(!is.na(as.numeric(as.character(.)))), as.numeric)

```


- **Self Defined Descriptive statistics**

To better describe the numerical data from the original measurements, we define a function that provides basic information such as sample size, mean, standard deviation, skewness, and kurtosis.

1. Sample Size (n): Each variable has 1001 observations, indicating a substantial amount of data.

2. Mean: The means of the variables range from around 24 to 36, suggesting the data are centered around these values. The temperature-related variables generally have means around 35, while 'Humidity' has a lower mean (28.76), and 'Distance' has a very low mean (0.731).

3. Standard Deviation (stdev): The standard deviation values indicate the spread of the data. For most temperature-related variables, the standard deviations are around 0.5 to 0.9, indicating relatively low variability. 'Humidity' has a higher variability (stdev of 13.07), which is notable. 'Distance' also shows substantial variability (stdev of 2.48).

4. Skewness: Positive skewness values indicate a right-skewed distribution (e.g., T_offset1, Max1R13_1, etc.). Negative skewness values indicate a left-skewed distribution (e.g., aveAllR13_1, T_FHCC1). Most temperature variables show slight to moderate skewness, indicating some asymmetry in the data distribution.

5. Kurtosis: Kurtosis values close to 3 indicate a normal distribution. Values less than 3 indicate a flatter distribution (platykurtic), while values greater than 3 indicate a more peaked distribution (leptokurtic). The kurtosis values in this dataset range from around 1.64 to 4.88, with most being around 2, suggesting generally platykurtic distributions, except T_FHTC1 which is leptokurtic.

6. Outliers: High skewness and kurtosis in some variables (e.g., T_FHTC1 with kurtosis of 4.88 and skewness of -1.33, Distance with kurtosis of 992.51 and skewness of 31.51) suggest the presence of outliers. 


```{r echo=FALSE, warning=FALSE, message=FALSE}
# Basic descriptive statistics 
des_stats <- function(x, na.omit=FALSE){
  if (na.omit)
    x <- x[!is.na(x)]
  m <- mean(x)
  n <- length(x)
  s <- sd(x)
  skew <- sum((x-m)^3/s^3)/n
  kurt <- sum((x-m)^4/s^4)/n - 3
  return(c(n=n, mean=m, stdev=s,
           skew=skew, kurtosis=kurt))
}
sapply(mydataframe[1:29], des_stats)
```


- **Plot Numerical Data**

The plot of the numerical data confirms the findings from the descriptive statistics. For example, the distribution of the 'Distance' variable is skewed by outliers.


```{r echo=FALSE, warning=FALSE, message=FALSE}
i<-1
e<-3
while(i<28){
    plots <- list()
    vars_plot <- names(mydataframe)[i:(i+2)]
    for (var in vars_plot) {
      p <- ggplot(mydataframe, aes_string(x = var)) +
        geom_histogram(aes(y = ..density..), binwidth = 0.1, fill = "blue", alpha = 0.5) +
        geom_density(alpha = .3, fill = "#FF6666") +
        labs(title = paste("Distribution of", var), x = var, y = "Density") +
         theme(
          plot.title = element_text(size = 8),  # Smaller plot title
          axis.title = element_text(size = 6),  # Smaller axis titles
          axis.text = element_text(size = 4)    # Smaller axis text
        )
      
      plots[[var]] <- p
    }
    
    # Combine the plots using patchwork
    combined_plot <- wrap_plots(plots) + plot_layout(ncol = 3)
    
    # Print the combined plot
    print(combined_plot)
    i<-i+3
}
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
plots <- list()
vars_plot <- names(mydataframe)[28:29]
for (var in vars_plot) {
  p <- ggplot(mydataframe, aes_string(x = var)) +
    geom_histogram(aes(y = ..density..), bin=30, fill = "blue", alpha = 0.5) +
    geom_density(alpha = .3, fill = "#FF6666") +
    labs(title = paste("Distribution of", var), x = var, y = "Density")+
  theme(
      plot.title = element_text(size = 8),  # Smaller plot title
      axis.title = element_text(size = 6),  # Smaller axis titles
      axis.text = element_text(size = 4)    # Smaller axis text
    )
  
  plots[[var]] <- p
}

# Combine the plots using patchwork
combined_plot <- wrap_plots(plots) + plot_layout(ncol = 3)

# Print the combined plot
print(combined_plot)
```

- **Dealing the Outlier**

To mitigate the impact of outliers, we consider filtering the data by setting lower and upper bounds based on the distance from the quantiles.


Interquartile Range (IQR)
$$
\text{IQR} = Q3 - Q1
$$

Lower Bound
$$
lower_-bound = Q1 - 1.5 \times IQR
$$

Upper Bound
$$
upper_-bound = Q3 + 1.5 \times IQR
$$


```{r echo=FALSE, warning=FALSE, message=FALSE}
# Define a function to detect outliers
detect_outliers <- function(x) {
  Q1 <- quantile(x, 0.25, na.rm = TRUE)
  Q3 <- quantile(x, 0.75, na.rm = TRUE)
  IQR <- Q3 - Q1
  lower_bound <- Q1 - 1.5 * IQR
  upper_bound <- Q3 + 1.5 * IQR
  x < lower_bound | x > upper_bound
}


# Function to remove outliers for specified variables
remove_outliers <- function(df, variables) {
  for (var in variables) {
    if (var %in% names(df)) {
      outliers <- detect_outliers(df[[var]])
      df <- df[!outliers, ]
    }
  }
  return(df)
}
columns_to_clean <- names(mydataframe)[1:29]
mydataframe <- remove_outliers(mydataframe, columns_to_clean)

```


- **Self Defined Descriptive statistics w/o Outliers **

After removing the outliers from the dataset, 675 observations remain.

1. The skewness values are close to zero, indicating that the distribution is symmetrical with no significant skew to either the left or the right.

2. A kurtosis value close to zero suggests that the tails of the distribution are similar to those of a normal distribution, exhibiting neither heavy-tailed (leptokurtic) nor light-tailed (platykurtic) behavior.


```{r echo=FALSE, warning=FALSE, message=FALSE}
sapply(mydataframe[1:29], des_stats)
```


- **Plot Numerical Data w/o Outliers**

The plot of the numerical data confirms the findings of the descriptive statistics. The plots exhibit concentrated ranges.


```{r echo=FALSE, warning=FALSE, message=FALSE}
i<-1
e<-3
while(i<28){
    plots <- list()
    vars_plot <- names(mydataframe)[i:(i+2)]
    for (var in vars_plot) {
      p <- ggplot(mydataframe, aes_string(x = var)) +
        geom_histogram(aes(y = ..density..), binwidth = 0.1, fill = "blue", alpha = 0.5) +
        geom_density(alpha = .3, fill = "#FF6666") +
        labs(title = paste("Distribution of", var), x = var, y = "Density") +
         theme(
          plot.title = element_text(size = 8),  # Smaller plot title
          axis.title = element_text(size = 6),  # Smaller axis titles
          axis.text = element_text(size = 4)    # Smaller axis text
        )
      
      plots[[var]] <- p
    }
    
    # Combine the plots using patchwork
    combined_plot <- wrap_plots(plots) + plot_layout(ncol = 3)
    
    # Print the combined plot
    print(combined_plot)
    i<-i+3
}
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
plots <- list()
vars_plot <- names(mydataframe)[28:29]
for (var in vars_plot) {
  p <- ggplot(mydataframe, aes_string(x = var)) +
    geom_histogram(aes(y = ..density..), bin=30, fill = "blue", alpha = 0.5) +
    geom_density(alpha = .3, fill = "#FF6666") +
    labs(title = paste("Distribution of", var), x = var, y = "Density")+
  theme(
      plot.title = element_text(size = 8),  # Smaller plot title
      axis.title = element_text(size = 6),  # Smaller axis titles
      axis.text = element_text(size = 4)    # Smaller axis text
    )
  
  plots[[var]] <- p
}

# Combine the plots using patchwork
combined_plot <- wrap_plots(plots) + plot_layout(ncol = 3)

# Print the combined plot
print(combined_plot)
```
- **Correlation of Numerical Data**

We also want to examine the correlations among the dependent variables. A correlation of 1 indicates the strongest positive relationship, -1 indicates the strongest negative relationship, and 0 indicates no relationship.


```{r  echo=FALSE, warning=FALSE, message=FALSE, results='hide'}

# Correlation matrix and tests of significance via corr.test() in psych package
corr.test(mydataframe[1:29], use="complete")
```
- **Correlation Matrix Heatmap**

```{r echo=FALSE, warning=FALSE, message=FALSE}
corr_result <- corr.test(mydataframe[1:29], use = "complete")

corr_matrix <- corr_result$r
# wider to long form
corr_matrix_melt <- melt(corr_matrix)

ggplot(data = corr_matrix_melt, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() + #heat map tile
  scale_fill_gradient2(low = "blue", high = "red", mid = "white",
                       midpoint = 0, limit = c(-1,1), space = "Lab",
                       name="Correlation") +
  theme_minimal() + 
  theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 12, hjust = 1)) +
  coord_fixed() + #Ensures that each tile is square by fixing the aspect ratio.
  labs(title = "Correlation Heatmap", x = "", y = "") +
  theme(axis.text.x = element_blank(),
        axis.text.y = element_blank())

```

- **Oral Temperature VS. Dependent Variables**

We want to examine the relationship between the target value and each individual dependent variable, without considering the other dependent variables.


```{r echo=FALSE, warning=FALSE, message=FALSE}
i<-1
e<-3
while(i<25){
    plots <- list()

    # plot target vs dependent variables
    devars <- names(mydataframe)[i:(i+2)]
    for (var in devars) {
      p <- ggplot(mydataframe, aes_string(x = var, y="T_OR_Max1")) +
        geom_point() +
        geom_smooth(method = "lm", se = TRUE) +
        labs(title = paste("T_OR_Max1 vs.", var), x = var, y = "T_OR_Max1")+
         theme(
          plot.title = element_text(size = 8),  # Smaller plot title
          axis.title = element_text(size = 6),  # Smaller axis titles
          axis.text = element_text(size = 4)    # Smaller axis text
        )
      
      plots[[var]] <- p
    }
     # Combine the plots using patchwork
    combined_plot <- wrap_plots(plots) + plot_layout(ncol = 3)
    
    # Print the combined plot
    print(combined_plot)
    i<-i+3
}

```

```{r echo=FALSE, warning=FALSE, message=FALSE}
plots <- list()
devars <- names(mydataframe)[27:29]
    for (var in devars) {
      p <- ggplot(mydataframe, aes_string(x = var, y="T_OR_Max1")) +
        geom_point() +
        geom_smooth(method = "lm", se = TRUE) +
        labs(title = paste("T_OR_Max1 vs.", var), x = var, y = "T_OR_Max1")+
         theme(
          plot.title = element_text(size = 8),  # Smaller plot title
          axis.title = element_text(size = 6),  # Smaller axis titles
          axis.text = element_text(size = 4)    # Smaller axis text
        )
      
      plots[[var]] <- p
    }
     # Combine the plots using patchwork
    combined_plot <- wrap_plots(plots) + plot_layout(ncol = 3)
    
    # Print the combined plot
    print(combined_plot)
```

- **Standardization of Data**

The purpose of standardization is to ensure equal weightage of features. We use the z-score to standardize the data, giving it a mean of 0 and a standard deviation of 1.


$$
Standardized Value = \frac{x - \mu}{\sigma}
$$


```{r echo=FALSE, warning=FALSE, message=FALSE}
# standardize variables
mydataframe[, 1:29] <- scale(mydataframe[, 1:29])
mydataframe_copy <- mydataframe
```

# Modeling:

- **Split Data for Training and Testing by the Ration 80:20**

By setting the training/testing ratio to 80/20, we aim to ensure there is sufficient data to fit the model while retaining enough unseen data to test the created model.


```{r echo=FALSE, warning=FALSE, message=FALSE}
set.seed(42)
split <- sample.split(mydataframe$T_OR_Max1, SplitRatio = 0.8)
train_data <- subset(mydataframe, split == TRUE)
test_data <- subset(mydataframe, split == FALSE)

```
- **Full Liner Regression Model**

We begin with a full linear model using all the available predictors.

1. Significant predictors: 
  a. T_offset1: Negative impact on T_OR_Max1 (p-value = 0.00572).
  b. T_RC_Wet1: Positive impact (p-value = 0.01794).
  c. T_FHBC1: Positive impact (p-value = 0.00447).
  d. T_Max1: Highly significant positive impact (p-value < 2e-16).
  e. T_atm: Positive impact (p-value = 0.04110).
  f. Humidity: Positive impact (p-value = 0.04047).
  
2. Model Performance:
  a. Multiple R-squared of 0.4829 indicates that approximately 48.29% of the variability in T_OR_Max1 is explained by the model.
  b. Adjusted R-squared of 0.4545 accounts for the number of predictors in the model, providing a more conservative estimate.
  c. Since the value of R-squared is less than 0.5, we want to improve our model performance. 

```{r echo=FALSE, warning=FALSE, message=FALSE, results='hide'}
# Full regression model
fit_full <- lm(T_OR_Max1~. ,data=train_data)
```
```{r echo=FALSE, warning=FALSE, message=FALSE}
summary(fit_full)
```

- **Full Regression and Stepwised Regression Model **

1. To build a comprehensive regression model, we add interaction terms (for two predictors only) to the full linear model. This increases the R-squared value, bringing it closer to 1. However, it also significantly increases the model's complexity. In our case, with 435 factors (29 dependent variables and 406 interaction terms) involved, this suggests potential overfitting, where the model fits the training data very well but may not generalize well to new, unseen data.

2.A stepwise method (backward) provides a way to remove predictors from the full regression model, reducing complexity. However, the resulting model formula remains overly complicated. 


- **Improved Regression Model**

1. We begin with the full linear regression model.

2. Using the stepwise method, we removed predictors from the full linear regression model. The predictors retained in the model are: T_offset1, aveAllR13_1, aveAllL13_1, T_RC_Wet1, T_LC_Max1, RCC1, canthi4Max1, T_FHCC1, T_FHLC1, T_FHBC1, T_FH_Max1, T_Max1, T_atm, Humidity, and Distance.

3. Add interaction terms for the retained predictors.

4. Run the stepwise method again to determine the final model.

```{mermaid}
flowchart TB
  A(Full Linear Regression) --> B(Stepwise Regression)
  B --> C(Add Interaction Terms)
  C --> D(2nd Stepwise Regression)
  D --> E(Final model)
```

```{r echo=FALSE, warning=FALSE, message=FALSE, results='hide'}
# Reduced regression model with interaction terms
fit_reduce_inter <- lm(T_OR_Max1 ~ T_offset1 + Max1R13_1 + aveAllR13_1 + 
    T_RC_Dry1 + T_RC_Wet1 + canthi4Max1 + T_FHBC1 + T_FH_Max1 + 
    T_Max1 + T_atm + Humidity + T_offset1:aveAllR13_1 +
    T_offset1:T_FHBC1 + T_offset1:T_atm + aveAllR13_1:T_FHBC1 +
    aveAllR13_1:Humidity + T_RC_Dry1:canthi4Max1 + 
    T_RC_Dry1:T_FHBC1 + T_RC_Dry1:T_Max1 + T_RC_Dry1:Humidity +
    T_RC_Wet1:canthi4Max1 + T_RC_Wet1:T_Max1 + T_RC_Wet1:T_atm +
    T_RC_Wet1:Humidity + canthi4Max1:T_FH_Max1 + T_FHBC1:Humidity +
    T_FH_Max1:T_Max1 + T_FH_Max1:T_atm + T_Max1:T_atm, 
    data = train_data)
```
- **Summary of the Improved Regression Model**
  
1. The **predictors** kept are: T_offset1 ,Max1R13_1, aveAllR13_1, T_RC_Dry1, T_RC_Wet1, canthi4Max1, T_FHBC1, T_FH_Max1, T_Max1, and T_atm. 
    
2. The **interaction terms** kept are: T_offset1:aveAllR13_1, T_offset1:T_FHBC1, 
T_offset1:T_atm, aveAllR13_1:T_FHBC1, aveAllR13_1:Humidity, T_RC_Dry1:canthi4Max1, T_RC_Dry1:T_FHBC1, T_RC_Dry1:T_Max1, T_RC_Dry1:Humidity, T_RC_Wet1:canthi4Max1, T_RC_Wet1:T_Max1, T_RC_Wet1:T_atm, T_RC_Wet1:Humidity, canthi4Max1:T_FH_Max1, T_FHBC1:Humidity, T_FH_Max1:T_Max1, T_FH_Max1:T_atm, and T_Max1:T_atm.    

3. The Multiple R-squared of 0.5667 indicates that approximately 56.67% of the variability in T_OR_Max1 is explained by the model.

4. The F-statistic and associated p-value (< 2.2e-16) indicate that the overall model is statistically significant.

5. Variance Inflation Factor (VIF) values indicate the degree of multicollinearity in the model. High VIF values in this model, such as T_RC_Dry1 of 41.72 and Max1R13_1 of 41.99, suggest that predictors has a high correlation with other predictors. 

6. To address multicollinearity effectively in the regression models., we will try applying PCA, or by using Ridge regression. 

```{r echo=FALSE, warning=FALSE, message=FALSE}
summary(fit_reduce_inter)
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
vif_values <- vif(fit_reduce_inter)
cat("\n")
print(vif_values)
```

- **Address high VIF for the Model with PCA**

Principal Component Analysis (PCA) transforms the predictors into a set of uncorrelated components, reducing the number of variables without losing much information. Using too many predictors can lead to overfitting and complicate the interpretation of the analysis.


```{r echo=FALSE, warning=FALSE, message=FALSE}
# PCA
mydataframe_pca <- mydataframe_copy
# create interactions
mydataframe_pca <- mydataframe_pca %>%
  mutate(
    T_offset1_aveAllR13_1=T_offset1*aveAllR13_1,
    T_offset1_T_FHBC1=T_offset1*T_FHBC1, 
    T_offset1_T_atm=T_offset1*T_atm,
    aveAllR13_1_T_FHBC1=aveAllR13_1*T_FHBC1,
    aveAllR13_1_Humidity=aveAllR13_1*Humidity,
    T_RC_Dry1_canthi4Max1=T_RC_Dry1*canthi4Max1,
    T_RC_Dry1_T_FHBC1=T_RC_Dry1*T_FHBC1,
    T_RC_Dry1_T_Max1=T_RC_Dry1*T_Max1,
    T_RC_Dry1_Humidity=T_RC_Dry1*Humidity,
    T_RC_Wet1_canthi4Max1=T_RC_Wet1*canthi4Max1,
    T_RC_Wet1_T_Max1=T_RC_Wet1*T_Max1,
    T_RC_Wet1_T_atm=T_RC_Wet1*T_atm,
    T_RC_Wet1_Humidity=T_RC_Wet1*Humidity,
    canthi4Max1_T_FH_Max1=canthi4Max1*T_FH_Max1,
    T_FHBC1_Humidity=T_FHBC1*Humidity,
    T_FH_Max1_T_Max1=T_FH_Max1*T_Max1,
    T_FH_Max1_T_atm=T_FH_Max1*T_atm,
    T_Max1_T_atm=T_Max1*T_atm
  )

reduced_columns_pca <- mydataframe_pca %>% 
  select(T_offset1,Max1R13_1, aveAllR13_1, T_RC_Dry1, T_RC_Wet1, 
         canthi4Max1, T_FHBC1, T_FH_Max1, T_Max1, T_atm, T_offset1_aveAllR13_1,
         T_offset1_T_FHBC1, 
         T_offset1_T_atm,
         aveAllR13_1_T_FHBC1,
         aveAllR13_1_Humidity,
         T_RC_Dry1_canthi4Max1,
         T_RC_Dry1_T_FHBC1,
         T_RC_Dry1_T_Max1,
         T_RC_Dry1_Humidity,
         T_RC_Wet1_canthi4Max1,
         T_RC_Wet1_T_Max1,
         T_RC_Wet1_T_atm,
         T_RC_Wet1_Humidity,
         canthi4Max1_T_FH_Max1,
         T_FHBC1_Humidity,
         T_FH_Max1_T_Max1,
         T_FH_Max1_T_atm,
         T_Max1_T_atm)

```

1. As shown in the scree plot, the first six components have eigenvalues greater than 1. The curve begins to flatten at the 7th component (elbow), indicating that the first six components should be retained.

```{r echo=FALSE, warning=FALSE, message=FALSE, , results='hide'}
fa.parallel(reduced_columns_pca, fa = "pc", n.iter = 100,
            show.legend = FALSE, main = "Scree plot with parallel analysis")
abline(h=1)
```

2.The first six components contribute to more than 75% of the total variance.

```{r echo=FALSE, warning=FALSE, message=FALSE}
# Perform PCA using prcomp
pca_result <- prcomp(reduced_columns_pca,
                     center = FALSE, # Data already centered
                     scale. = FALSE) # Data already scaled

# Extract the eigenvalues from the PCA result
eigenvalues <- pca_result$sdev^2

# Print the eigenvalues
# print(paste("Component eigenvalues: ", round(eigenvalues, 2)))

# Calculate the percentage contributions for each principal component
percentage_contributions <- eigenvalues / sum(eigenvalues) * 100

# Print the percentage contributions
# print(paste("Component contributions: ", round(percentage_contributions, 2), "%"))

# Print accumulated contributions
acc_con <- 0
for (i in seq_along(percentage_contributions)) {
  acc_con <- acc_con + percentage_contributions[i]
  print(paste("Accumulated contribution up to component", i, ": ", round(acc_con, 2), "%"))
}

```

3. The contributions of the first six components are also shown in the plot.

```{r echo=FALSE, warning=FALSE, message=FALSE}
# Create a data frame for plotting
principal_components <- paste0("PC", 1:length(percentage_contributions))
contributions_df <- data.frame(
  Principal_Component = factor(principal_components, levels = principal_components),
  Percentage_Contribution = percentage_contributions
)

# Plot the percentage contributions
ggplot(contributions_df, aes(x = Principal_Component, y = Percentage_Contribution, fill = Principal_Component)) +
  geom_bar(stat = "identity") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 12, hjust = 1),
        legend.position = "none")+
  labs(title = "Percentage Contributions of Principal Components",
       x = "Principal Component",
       y = "Percentage Contribution")
```

4. The linear model with six components shows great VIF values, close to 1 (all less than 5). However, the R-squared value drops to 0.321, meaning only 32.1% of the variations are explained. Adding up to 20 components increases the R-squared value to over 0.5, but this makes the model too complex to use.


```{r echo=FALSE, warning=FALSE, message=FALSE}
# Extracting the first 6 principal components
pc_data <- pca_result$x[, 1:6]

reduced_columns_target<- mydataframe_pca %>% 
  select(T_OR_Max1)


# modeling
lm_data <- cbind(pc_data, reduced_columns_target)
set.seed(42)
split <- sample.split(lm_data$T_OR_Max1, SplitRatio = 0.8)
train_data_pca <- subset(lm_data, split == TRUE)
test_data_pca <- subset(lm_data, split == FALSE)

fit_pca <- lm(T_OR_Max1~. ,data=train_data_pca)
summary(fit_pca)
vif_values_pca <- vif(fit_pca)
print(vif_values_pca)
```

- **Address high VIF with Ridge Regression**

Ridge Regression adds a penalty equal to the square of the magnitude of the coefficients, helping to reduce the impact of multicollinearity.

```{r echo=FALSE, warning=FALSE, message=FALSE}
# Prepare the data
x_train <- model.matrix(T_OR_Max1 ~ T_offset1 + Max1R13_1 + aveAllR13_1 +                                     T_RC_Dry1 + T_RC_Wet1 + canthi4Max1 + T_FHBC1 + T_FH_Max1+
                          T_Max1 + T_atm + Humidity + T_offset1:aveAllR13_1 +
                         T_offset1:T_FHBC1 + 
                         T_offset1:T_atm + aveAllR13_1:T_FHBC1 + 
                         aveAllR13_1:Humidity +
                         T_RC_Dry1:canthi4Max1 + T_RC_Dry1:T_FHBC1 +
                         T_RC_Dry1:T_Max1 + 
                         T_RC_Dry1:Humidity + T_RC_Wet1:canthi4Max1 +
                         T_RC_Wet1:T_Max1 + 
                         T_RC_Wet1:T_atm + T_RC_Wet1:Humidity + 
                         canthi4Max1:T_FH_Max1 + 
                         T_FHBC1:Humidity + T_FH_Max1:T_Max1 + T_FH_Max1:T_atm +
                         T_Max1:T_atm, train_data)[,-1] 
y_train <- train_data$T_OR_Max1

# Ridge Regression
set.seed(42)
ridge_model <- cv.glmnet(x_train, y_train, alpha = 0) # alpha = 0 for ridge
best_lambda_ridge <- ridge_model$lambda.min

# Fit the final ridge model
ridge_final <- glmnet(x_train, y_train, alpha = 0, lambda = best_lambda_ridge)

# Extract coefficients
ridge_coefficients <- coef(ridge_final)


# Print coefficients
# print(ridge_coefficients)

# Construct the equation
intercept <- ridge_coefficients[1] # Intercept
coefficients <- ridge_coefficients[-1] # Coefficients for predictors
predictor_names <- colnames(x_train)

# Display the equation
cat("Ridge Regression Equation:\n")
cat("T_OR_Max1 =", intercept, "+\n")
for (i in 1:length(coefficients)) {
  cat(sprintf("(%.4f * %s)", coefficients[i], predictor_names[i]))
  if (i < length(coefficients)) {
    cat(" +\n")
  } else {
    cat("\n")
  }
}
```

# Evaluate the model

1. Root Mean Squared Error (RMSE) is a common metric used to evaluate the performance of regression models. It measures the average magnitude of the errors between the predicted and actual values. The formula for RMSE is:

$$
RMSE = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (\hat{y_i} - y_i)^2}
$$

2. How you can calculate RMSE for training and testing data:

  a. Train your regression model using the training data.
  b. Make predictions on both the training and testing datasets.
  c. Calculate RMSE using the actual and predicted values for both datasets.
  

3. We also calculate R Squared to see how many percentage of the variability in T_OR_Max1 is explained by the model. The formular for R Squared is:

$$
R^2 = 1 - \frac{RSS}{TSS}
$$


4. Model Accuracy: RMSE values close to zero indicate a better fit. In this case, the RMSE values are relatively low, suggesting that the model's predictions are close to the actual values.


5. Model Performance: The RMSE on the training data (0.7212) is very close to the RMSE on the testing data (0.7401). This indicates consistent performance across both datasets, suggesting that the model is well-generalized and not overfitting. R Squared value shoes approximately 51.4% of the variability in T_OR_Max1 is explained by the model.


```{r echo=FALSE, warning=FALSE, message=FALSE}
# Predictions on training data
ridge_train_preds <- predict(ridge_final, s = best_lambda_ridge, newx = x_train)
ridge_train_rmse <- sqrt(mean((ridge_train_preds - y_train)^2))

# Calculate RSS and TSS for training data
rss_train <- sum((ridge_train_preds - y_train)^2) # Residual Sum of Squares
tss_train <- sum((y_train - mean(y_train))^2)    # Total Sum of Squares

# Calculate R-squared for training data
ridge_r2_train <- 1 - rss_train / tss_train


# Predictions on test data
x_test <- model.matrix(T_OR_Max1 ~ T_offset1 + Max1R13_1 + aveAllR13_1 +                                     T_RC_Dry1 + T_RC_Wet1 + canthi4Max1 + T_FHBC1 + T_FH_Max1 + 
                         T_Max1 + T_atm + Humidity + T_offset1:aveAllR13_1 +
                         T_offset1:T_FHBC1 + 
                         T_offset1:T_atm + aveAllR13_1:T_FHBC1 + 
                         aveAllR13_1:Humidity +
                         T_RC_Dry1:canthi4Max1 + T_RC_Dry1:T_FHBC1 +
                         T_RC_Dry1:T_Max1 + 
                         T_RC_Dry1:Humidity + T_RC_Wet1:canthi4Max1 +
                         T_RC_Wet1:T_Max1 + 
                         T_RC_Wet1:T_atm + T_RC_Wet1:Humidity + 
                         canthi4Max1:T_FH_Max1 + 
                         T_FHBC1:Humidity + T_FH_Max1:T_Max1 + T_FH_Max1:T_atm +
                         T_Max1:T_atm, test_data)[,-1]

y_test <- test_data$T_OR_Max1

ridge_test_preds <- predict(ridge_final, s = best_lambda_ridge, newx = x_test)
ridge_test_rmse <- sqrt(mean((ridge_test_preds - y_test)^2))


cat("Caculated value based on Ridge model:", "\n")
# Output R-squared
cat("R-squared :", ridge_r2_train, "\n")
# Output RMSE
cat("RMSE on Training Data:", ridge_train_rmse, "\n")
cat("RMSE on Testing Data:", ridge_test_rmse, "\n")

```
# Review the model

1. Model Structure:
The model predicts the variable T_OR_Max1 based on multiple predictors and their interactions. It is a ridge regression model, used to address multicollinearity among the predictors by adding a penalty to the size of the coefficients. This results in a more stable and generalizable model. 

2. Coefficient:
The coefficients have been shrunk towards zero to prevent overfitting.

3. Interaction Terms:
Interaction terms are included to capture the combined effects of predictors. Regularization helps manage the complexity introduced by these terms, ensuring the model remains interpretable and generalizable.

3. Regularization Term:
The 'glmnet' package in R is used to generate the ridge regression model. Cross-validation helps to find the optimal value of the regularization parameter 'lambda,' which controls the trade-off between fitting the training data and keeping the coefficients small.

# Reference
